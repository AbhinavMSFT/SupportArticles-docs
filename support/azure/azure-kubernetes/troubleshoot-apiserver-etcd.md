---
title: Troubleshoot API server and etcd issues in AKS
description: Provides a troubleshooting guide for API server and etcd issues in Azure Kubernetes Services.
author: merooney
ms.author: segule
ms.date: 07/19/2023
ms.service: azure-kubernetes-service
ms.reviewer: segule, merooney, v-weizhu
---
# Troubleshoot API server and etcd issues in Azure Kubernetes Services

This guide is designed to help you identify and resolve any unlikely issues you may encounter with the API server in large Azure Kubernetes Services (AKS) deployments.

Microsoft has tested the reliability and performance of the API server at a scale of 5,000 nodes and 65,000 pods, with the ability to automatically scale out and deliver [Kubernetes Service Level Objectives (SLOs)](https://github.com/kubernetes/community/blob/master/sig-scalability/slos/slos.md). If you experience high latencies or timeouts, it's likely due to a resource leakage on etcd or an offending client with excessive API calls.

## Prerequisites

- AKS diagnostics logs (specifically kube-audit events) have been enabled and sent to a [Log Analytics workspace](/azure/aks/monitor-apiserver).
- Ensure that you're using the Standard tier for AKS clusters. If you're using the Free tier, the API server and etcd come with limited resources. AKS clusters in the Free tier don't provide high availability, which is often the root cause of API server and etcd issues.

## Symptoms

The following table outlines the common symptoms of API server failures:

| Symptom | Description |
|---|---|
| Time-outs from the API server | Frequent time-outs that are beyond the guarantees in [the AKS API server SLA](/azure/aks/free-standard-pricing-tiers#uptime-sla-terms-and-conditions). `e.g. kubectl commands timeout` |
| High latencies | High latencies that make the Kubernetes SLOs fail. `e.g. kubectl command takes more than 30 seconds to list pods`|

## Causes

Here are the three most common causes of API server failures:

- A network rule blocks the traffic from agent nodes to the API server.
- A problematic client makes excessive `LIST` or `PUT` calls.
- A problematic client leaks etcd objects and results in a slowdown of etcd.

The following sections help you identify if the three most common causes of API server failures affect your cluster.

## Verify network connectivity between the API server and agent pool nodes
 
To validate whether a misconfigured network policy is blocking communication between the API server and agent pool system nodes, use the following command:

```azurecli
az vmss run-command invoke -g "myAgentpoolVmssResourceGroup" -n "myAgentpoolVmss" \
--command-id RunShellScript \
--instance-id 0 \
--scripts "echo | nc -vz "myAKSApiServerFQDN" 443"
```
If the command above returns `connection succeeded`, then the network connectivity is unimpeded.

## <a id="identifytopuseragents"></a> Identify top user agents by the number of requests

To identify which clients are generating the most requests, and potentially the most API server load, use a query like the following. This query lists the top 10 user agents by the number of API server requests sent:

```kusto
AzureDiagnostics
| where TimeGenerated between(now(-1h)..now()) // filter for the timeframe you experienced the problem
| where Category == "kube-audit" 
| extend event = parse_json(log_s) 
| extend User = tostring(event.user.username) 
| summarize count() by User 
| top 10 by count_ 
| project User, count_ 
```

While this information is helpful to know which clients generate the highest request volume, high request volume alone may not be a cause for concern. A better indicator of the actual load each client generates on the API server is the response latency they experience.

## Identify and chart the average latency of API server requests per user agent

To identify the average latency of API server requests per user agent plotted on a time chart, use the following query:

```kusto
AzureDiagnostics
| where TimeGenerated between(now(-1h)..now()) // filter for the timeframe you experienced the problem
| where Category == "kube-audit" 
| extend event = parse_json(log_s) 
| extend User = tostring(event.user.username)
| extend start_time = todatetime(event.requestReceivedTimestamp)
| extend end_time = todatetime(event.stageTimestamp)
| extend latency = datetime_diff('millisecond', end_time, start_time)
| summarize avg(latency) by User, bin(start_time, 5m) 
| render timechart 
```

This query is a follow-on to the query in [Identify top user agents by the number of requests](#identifytopuseragents) section. It may give you more insights into the actual load generated by each user agent over time.

> [!TIP]
> By analyzing this data, you can identify patterns and anomalies that may indicate issues with your AKS cluster or applications. For example, you might notice that a particular user is experiencing high latency. This could indicate the type of API calls that are causing excessive load on the API server or etcd.

## Identify bad API calls for a given user agent

Use the following query to tabulate the P99 latency of API calls across different resource types for a given client:

```kusto
AzureDiagnostics
| where TimeGenerated between(now(-1h)..now()) // filter for the timeframe you experienced the problem
| where Category == "kube-audit" 
| extend event = parse_json(log_s) 
| extend HttpMethod = tostring(event.verb) 
| extend Resource = tostring(event.objectRef.resource) 
| extend User = tostring(event.user.username) 
| where User == "DUMMYUSERAGENT" // filter by name of the useragent you are interested in
| where Resource != ""
| extend start_time = todatetime(event.requestReceivedTimestamp)
| extend end_time = todatetime(event.stageTimestamp)
| extend latency = datetime_diff('millisecond', end_time, start_time)
| summarize p99latency=percentile(latency, 99) by HttpMethod, Resource 
| render table  
```

The results from this query can be useful for identifying types of API calls that fail the upstream Kubernetes SLOs. In most cases, an offending client may be making too many `LIST` calls on a large set of objects or objects that are too large in size. Unfortunately, there are no hard scalability limits that can guide users on API server scalability. API server or etcd scalability limits depend on a variety of factors explained in [Kubernetes Scalability thresholds](https://github.com/kubernetes/community/blob/master/sig-scalability/configs-and-limits/thresholds.md). 

## Verify that clients don't leak resources in etcd

> [!TIP]
> Whenever an object in etcd is mutated, a new complete version of that object is created. Should the object being mutated be large, this can end up consuming a lot of space. To stop etcd fom reaching capacity and causing cluster downtime, you can cap the maximum number of resources created and/or slow the number of revisions generated for resource instances.
=======
A common issue is continuously creating objects without deleting unused ones in the etcd database. This can cause performance issues when dealing with too many objects of any type (> 10 kilobytes). A rapid increase of changes on such objects could also cause the etcd database size (4 gigabytes by default) to be exceeded.

To check the etcd database usage, navigate to **Diagnose and Solve problems** in the Azure portal. Run the Etcd Availability diagnosis tool by searching for "_etcd_" in the search box. The diagnosis tool shows you the usage breakdown and the total database size.

:::image type="content" source="media/troubleshoot-apiserver-etcd/etcd-detector.png" alt-text="Screenshot that shows the Etcd Availability Diagnosis for AKS.":::

Alternatively, if you just want a quick way to see current size of your etcd db in bytes:

```bash
kubectl get --raw /metrics | grep "etcd_db_total_size_in_bytes"
```

If you have identified objects that are no longer in use but are taking up resources, consider deleting them. For example, you can delete completed jobs to free up space:

```bash
kubectl delete jobs --field-selector status.successful=1
```

Alternatively, for objects that support [automatic clean-up](https://kubernetes.io/docs/concepts/architecture/garbage-collection/), you can set time to live (TTL) values to limit the lifetime of these objects. You can also label your objects so that you can bulk delete all objects of a specific type using label selectors. If you establish [owner references](https://kubernetes.io/docs/concepts/overview/working-with-objects/owners-dependents/) among objects, then when the parent object is deleted, any dependent objects will be automatically deleted.

If you'd like to limit the number of objects that can created, you can [define object quotas](https://kubernetes.io/docs/concepts/policy/resource-quotas/#object-count-quota)

## How to throttle a client overwhelming the control plane

If you determine that etcd isn't overloaded with too many objects, consider tuning your client's API call pattern to reduce the pressure on the control plane.

If you can't tune the client, you can use the [Priority and Fairness](https://kubernetes.io/docs/concepts/cluster-administration/flow-control/) feature in Kubernetes to throttle the client. This can help preserve the health of the control plane and prevent other applications from failing.

The following sample illustrates how to throttle a problematic client's LIST Pods API set to five concurrent calls:

1. Create a `FlowSchema` that matches the API call pattern of the problematic client:

    ```yaml
    apiVersion: flowcontrol.apiserver.k8s.io/v1beta2
    kind: FlowSchema
    metadata:
      name: restrict-bad-client
    spec:
      priorityLevelConfiguration:
        name: very-low-priority
      distinguisherMethod:
        type: ByUser
      rules:
      - resourceRules:
        - apiGroups: [""]
          namespaces: ["default"]
          resources: ["pods"]
          verbs: ["list"]
        subjects:
        - kind: ServiceAccount
          serviceAccount:
            name: bad-client-account
            namespace: default 
    ```

2. Create a lower priority configuration to throttle bad API calls of the client:

    ```yaml
    apiVersion: flowcontrol.apiserver.k8s.io/v1beta2
    kind: PriorityLevelConfiguration
    metadata:
      name: very-low-priority
    spec:
      limited:
        assuredConcurrencyShares: 5
        limitResponse:
          type: Reject
      type: Limited
    ```

3. Observe the throttled call in the API server metrics.

    ```bash
    kubectl get --raw /metrics | grep "restrict-bad-client"
    ```

[!INCLUDE [Azure Help Support](../../includes/azure-help-support.md)]
