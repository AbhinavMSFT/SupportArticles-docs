---
title: Troubleshoot API server and etcd issues #Required; this page title is displayed in search results; Always include the word "troubleshoot" in this line.
description: Troubleshooting guide for API server and etcd in Azure Kubernetes Services #Required; this article description is displayed in search results.
author: seguler #Required; your GitHub user alias — correct capitalization is needed.
ms.author: segule #Required; Microsoft alias of the author.
ms.topic: troubleshooting #Required.
ms.date: 6/30/2023 #Required; enter the date in the mm/dd/yyyy format.
ms.service: azure-kubernetes-service
---


# Troubleshoot your AKS API server

This guide aims to assist users in identifying and resolving unlikely issues encountered with the API Server in large AKS deployments.

When considering the resilience of the API server we have tested reliability and performance to a scale of 5,000 nodes and 65,000 pods, with the ability to automatically scale out and deliver [Kubernetes SLOs][K8s SLOs]. As such, if you observe high latencies or timeouts, these are likely due to a resource leakage on etcd or an offending client with excessive heavy API calls.


<!---Avoid notes, tips, and important boxes—readers tend to skip over them. It's better to put those things directly into the text of the article. --->

## Prerequisites

- AKS Diagnostics logs have been enabled and sent to a [Log Analytics workspace][log-analytics-workspace-overview].
- Ensure that you're using the Standard tier of AKS. If you're on the Free tier, the API server and Etcd come with limited resources. Free tier clusters don't provide high availability, which is often the root cause of API server and Etcd issues. 

## Symptoms

The following table outlines the common symptoms of API server failures.

| Symptom | Description |
|---|---|
| Timeouts from API server | Frequent timeouts that are beyond what's guaranteed in [the AKS API Server SLA][apiserversla] |
| High latencies | High latencies that fail the Kubernetes SLOs |


## Troubleshooting Walkthrough

The following sections help you identify if these three most common causes for failures affect your cluster:
- A network rule blocking traffic to API Server from agent nodes
- An offending client is making excessive LIST or PUT calls
- An offending client leaks Etcd objects and results in slowdown of Etcd

### Verify network connectivity between the API server and agent pool nodes

This command can be used to validate whether a misconfigured network policy is blocking communication between the API server and agent pool system nodes. If the following command returns connection succeeded, then connectivity is unimpeded.

```azurecli
az vmss run-command invoke -g "myAgentpoolVmssResourceGroup" -n "myAgentpoolVmss" \
--command-id RunShellScript \
--instance-id 0 \
--scripts "echo | nc -vz "myAKSApiServerFQDN" 443"
```

### Identify top user agents by number of requests

It can be useful to identify which clients are generating the most requests, and potentially the most API Server load. Below you find a query that lists the top 10 user agents by number of API server requests sent. 

```kusto
AzureDiagnostics
| where TimeGenerated between(now(-1h)..now()) // filter for the timeframe you experienced the problem
| where Category == "kube-audit" 
| extend event = parse_json(log_s) 
| extend User = tostring(event.user.username) 
| summarize count() by User 
| top 10 by count_ 
| project User, count_ 
```

This information is useful to get a sense of which clients have the greatest request volume. However, high request volume alone isn't necessarily a cause for concern. The response latency each client experiences is a better indicator of the true load they're generating on the API server.

### Chart Average Latency of Requests per User Agent

This query identifies the average latency of API server requests per user agent plotted on a time chart. This query is a follow-on to the previous query that may lend more insights into the true load over time generated by each user agent.

Troubleshooting tip: By analyzing this data, you can identify patterns and anomalies that may indicate issues with your Kubernetes cluster or applications. For example, you might notice that a particular user is experiencing high latency. This could indicate the type of API call that is causing excessive load on either API server or etcd.

```kusto
AzureDiagnostics
| where TimeGenerated between(now(-1h)..now()) // filter for the timeframe you experienced the problem
| where Category == "kube-audit" 
| extend event = parse_json(log_s) 
| extend User = tostring(event.user.username)
| where User == "DUMMYUSERAGENT" // filter by name of the useragent you are interested in
| extend start_time = todatetime(event.requestReceivedTimestamp)
| extend end_time = todatetime(event.stageTimestamp)
| extend latency = datetime_diff('millisecond', end_time, start_time)
| summarize avg(latency) by User, bin(start_time, 1m) 
| render timechart 
```


This query is used to tabulate the distribution of api calls across different resource types in a chart for a given client.

Troubleshooting tip:  The results from this query can be useful for identifying types of API calls that may be causing the problems. In most cases, an offending client may be making too many LIST calls on a large set of objects or large objects in size. Unfortunately, there aren't hard scalability limits that can guide users on API server scalability. API server or Etcd scalability limits depend on variety of factors explained in the [Kubernetes Scalability documents](https://github.com/kubernetes/community/blob/master/sig-scalability/configs-and-limits/thresholds.md). 


```kusto
AzureDiagnostics
| where TimeGenerated between(now(-1h)..now()) // filter for the timeframe you experienced the problem
| where Category == "kube-audit" 
| extend event = parse_json(log_s) 
| extend HttpMethod = tostring(event.verb) 
| extend Resource = tostring(event.objectRef.resource) 
| extend User = tostring(event.user.username) 
| where Resource != ""
| summarize count() by User, HttpMethod, Resource 
| render table  
```

#### How do you address a client overloading control plane?

1. Disable the client or tune its API call pattern.
2. Use Priority & Fairness feature to assign a lower priority for the client's API calls and throttle it to protect other applications. 

i. Create a FlowSchema that matches the offending client's API call pattern:

```yaml
apiVersion: flowcontrol.apiserver.k8s.io/v1beta2
kind: FlowSchema
metadata:
  name: restrict-bad-client
spec:
  priorityLevelConfiguration:
    name: restrict-bad-client
  distinguisherMethod:
    type: ByUser
  rules:
  - resourceRules:
    - apiGroups: [""]
      namespaces: ["default"]
      resources: ["pods"]
      verbs: ["list"]
    subjects:
    - kind: ServiceAccount
      serviceAccount:
        name: bad-client-account
        namespace: default 
```

ii. Create a lower priority configuration to throttle bad client's API calls

```yaml
apiVersion: flowcontrol.apiserver.k8s.io/v1beta2
kind: PriorityLevelConfiguration
metadata:
  name: very-low-priority
spec:
  limited:
    assuredConcurrencyShares: 5
    limitResponse:
      type: Reject
  type: Limited
```

ii. Then, you can observe the throttled call in the API server metrics.

```bash
kubectl get --raw /metrics | grep "restrict-bad-client"
```

### Verify your clients don't leak resources in Etcd

A common issue is to continuously create objects without deleting unused ones in the Etcd database. The etcd database is prone to performance issues when dealing with too many objects of any type (>10K). A rapid increase of changes on such objects could also result in exceeding the etcd database size (4 GB by default).

To check for Etcd database usage, go to Diagnose and Solve blade on the Azure Portal. Run the Etcd Availability diagnosis by searching for Etcd in the search box. The diagnosis shows you usage breakdown and the total database size. 


:::image type="content" source="media/troubleshoot-apiserver-etcd/etcd-detector.png" alt-text="Etcd Availability Diagnosis for AKS":::

If you have identified objects that are no longer in use but are taking resources, consider deleting them. As an example, completed jobs can be deleted to free up space:

```bash
kubectl delete jobs --field-selector status.successful=1
```

## References
[Query and Logging API server][monitor-aks-reference.md]

[Priority and Fairness][priority-and-fairness]


<!-- LINKS - external -->
[kube-audit-overview]: https://kubernetes.io/docs/tasks/debug/debug-cluster/audit/
[kube-apiserver-overview]: https://kubernetes.io/docs/reference/command-line-tools-reference/kube-apiserver/
[too-many-requests-error-tsg]: https://learn.microsoft.com/troubleshoot/azure/azure-kubernetes/429-too-many-requests-errors
[monitorr-apiserver]: https://github.com/MicrosoftDocs/azure-docs-pr/blob/main/articles/aks/monitor-apiserver.md
[priority-and-fairness]: https://kubernetes.io/docs/concepts/cluster-administration/flow-control/
[K8s SLOs]: https://github.com/kubernetes/community/blob/master/sig-scalability/slos/slos.md

<!-- LINKS - internal -->
[azure-diagnostics-overview]: ../azure-monitor/essentials/diagnostic-settings.md
[log-analytics-workspace-overview]: /azure/aks/monitor-aks
[design-log-analytics-deployment]: ../azure-monitor/logs/design-logs-deployment.md
[create-diagnostic settings]: ../azure-monitor/essentials/diagnostic-settings.md#create-diagnostic-settings
[cost-optimization-azure-monitor]: ../azure-monitor/best-practices-cost.md
[azure-diagnostics-table]: /azure/azure-monitor/reference/tables/azurediagnostics
[container-insights-overview]: ..//azure-monitor/containers/container-insights-overview.md
[apiserversla]: /azure/aks/free-standard-pricing-tiers#uptime-sla-terms-and-conditions