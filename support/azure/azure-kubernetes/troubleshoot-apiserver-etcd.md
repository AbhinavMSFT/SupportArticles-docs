---
title: Troubleshoot API server and etcd issues in AKS
description: Provides a troubleshooting guide for API server and etcd issues in Azure Kubernetes Services.
author: merooney
ms.author: segule
ms.date: 07/19/2023
ms.service: azure-kubernetes-service
ms.reviewer: segule, merooney, v-weizhu
---
# Troubleshoot API server and etcd issues in Azure Kubernetes Services

This guide is designed to help you identify and resolve any unlikely issues you may encounter with the API server in large Azure Kubernetes Services (AKS) deployments.

Considering the resilience of the API server, Microsoft has tested the reliability and performance of the API server at a scale of 5,000 nodes and 65,000 pods, with the ability to automatically scale out and deliver [Kubernetes Service Level Objectives (SLOs)](https://github.com/kubernetes/community/blob/master/sig-scalability/slos/slos.md). If you experience high latencies or time-outs, it's likely due to a resource leakage on etcd or a problematic client with excessive API calls.

## Prerequisites

- AKS diagnostic logs have been enabled and sent to a [Log Analytics workspace](/azure/aks/monitor-aks).
- Ensure that you're using the Standard tier for AKS clusters. If you're using the Free tier, the API server and etcd will have limited resources. AKS clusters in the Free tier don't provide high availability, which is often the root cause of API server and etcd issues.

## Symptoms

The following table outlines the common symptoms of API server failures:

| Symptom | Description |
|---|---|
| Time-outs from the API server | Frequent time-outs that are beyond the guarantees in [the AKS API server SLA](/azure/aks/free-standard-pricing-tiers#uptime-sla-terms-and-conditions). |
| High latencies | High latencies that make the Kubernetes SLOs fail.|

## Cause

Here are the three most common causes of API server failures:

- A network rule blocks the traffic from agent nodes to the API server.
- A problematic client makes excessive `LIST` or `PUT` calls.
- A problematic client leaks etcd objects and results in a slowdown of etcd.

The following sections help you identify if the three most common causes of API server failures affect your cluster.

## Verify network connectivity between the API server and agent pool nodes
 
To validate whether a misconfigured network policy is blocking communication between the API server and agent pool system nodes, use the following command:

```azurecli
az vmss run-command invoke -g "myAgentpoolVmssResourceGroup" -n "myAgentpoolVmss" \
--command-id RunShellScript \
--instance-id 0 \
--scripts "echo | nc -vz "myAKSApiServerFQDN" 443"
```
If the command above returns `connection succeeded``, then the network connectivity is unimpeded.

## <a id="identifytopuseragents"></a> Identify top user agents by the number of requests

To identify which clients are generating the most requests, and potentially the most API server load, use a query like the following. This query lists the top 10 user agents by the number of API server requests sent:

```kusto
AzureDiagnostics
| where TimeGenerated between(now(-1h)..now()) // filter for the timeframe you experienced the problem
| where Category == "kube-audit" 
| extend event = parse_json(log_s) 
| extend User = tostring(event.user.username) 
| summarize count() by User 
| top 10 by count_ 
| project User, count_ 
```

While this information is helpful to know which clients generate the highest request volume, high request volume alone may not be a cause for concern. A better indicator of the actual load each client generates on the API server is the response latency they experience.

## Identify and chart the average latency of API server requests per user agent

To identify the average latency of API server requests per user agent plotted on a time chart, use the following query:

```kusto
AzureDiagnostics
| where TimeGenerated between(now(-1h)..now()) // filter for the timeframe you experienced the problem
| where Category == "kube-audit" 
| extend event = parse_json(log_s) 
| extend User = tostring(event.user.username)
| extend start_time = todatetime(event.requestReceivedTimestamp)
| extend end_time = todatetime(event.stageTimestamp)
| extend latency = datetime_diff('millisecond', end_time, start_time)
| summarize avg(latency) by User, bin(start_time, 5m) 
| render timechart 
```

This query is a follow-on to the query in [Identify top user agents by the number of requests](#identifytopuseragents) section. It may give you more insights into the actual load generated by each user agent over time.

> [!TIP]
> By analyzing this data, you can identify patterns and anomalies that may indicate issues with your AKS cluster or applications. For example, you might notice that a particular user is experiencing high latency. This could indicate the type of API calls that are causing excessive load on the API server or etcd.

## Identify bad API calls for a given user agent

Use the following query to tabulate the P99 latency of API calls across different resource types for a given client:

```kusto
AzureDiagnostics
| where TimeGenerated between(now(-1h)..now()) // filter for the timeframe you experienced the problem
| where Category == "kube-audit" 
| extend event = parse_json(log_s) 
| extend HttpMethod = tostring(event.verb) 
| extend Resource = tostring(event.objectRef.resource) 
| extend User = tostring(event.user.username) 
| where User == "DUMMYUSERAGENT" // filter by name of the useragent you are interested in
| where Resource != ""
| extend start_time = todatetime(event.requestReceivedTimestamp)
| extend end_time = todatetime(event.stageTimestamp)
| extend latency = datetime_diff('millisecond', end_time, start_time)
| summarize p99latency=percentile(latency, 99) by HttpMethod, Resource 
| render table  
```

The results of this query can help identify types of API calls that fail the Upstream Kubernetes SLOs. In most cases, a problematic client may make too many `LIST` calls on a large set of objects or objects that are too large. Unfortunately, no hard scalability limits can guide users on API server scalability. API server or etcd scalability limits depend on various factors explained in [Kubernetes Scalability thresholds](https://github.com/kubernetes/community/blob/master/sig-scalability/configs-and-limits/thresholds.md). 

## Verify that clients don't leak resources in etcd

A common issue is continuously creating objects without deleting unused ones in the etcd database. This can cause performance issues when dealing with too many objects of any type (> 10 kilobytes). A rapid increase of changes on such objects could also cause the etcd database size (4 gigabytes by default) to be exceeded.

To check the etcd database usage, navigate to **Diagnose and Solve problems** in the Azure portal. Run the Etcd Availability diagnosis tool by searching for "_etcd_" in the search box. The diagnosis tool shows you the usage breakdown and the total database size.

:::image type="content" source="media/troubleshoot-apiserver-etcd/etcd-detector.png" alt-text="Screenshot that shows the Etcd Availability Diagnosis for AKS.":::

If you find objects that are no longer in use but are taking up resources, consider deleting them. For example, you can delete completed jobs to free up space:

```bash
kubectl delete jobs --field-selector status.successful=1
```

## How to address overload in the client control plane

If you determine that etcd isn't overloaded with too many objects, consider tuning your client's API call pattern to reduce the pressure on the control plane.

If you can't tune the client, you can use the [Priority and Fairness](https://kubernetes.io/docs/concepts/cluster-administration/flow-control/) feature in Kubernetes to throttle the client. This can help preserve the health of the control plane and prevent other applications from failing.

The following sample illustrates how to throttle a problematic client's LIST Pods API set to five concurrent calls:

1. Create a `FlowSchema` that matches the API call pattern of the problematic client:

    ```yaml
    apiVersion: flowcontrol.apiserver.k8s.io/v1beta2
    kind: FlowSchema
    metadata:
      name: restrict-bad-client
    spec:
      priorityLevelConfiguration:
        name: very-low-priority
      distinguisherMethod:
        type: ByUser
      rules:
      - resourceRules:
        - apiGroups: [""]
          namespaces: ["default"]
          resources: ["pods"]
          verbs: ["list"]
        subjects:
        - kind: ServiceAccount
          serviceAccount:
            name: bad-client-account
            namespace: default 
    ```

2. Create a lower priority configuration to throttle bad API calls of the client:

    ```yaml
    apiVersion: flowcontrol.apiserver.k8s.io/v1beta2
    kind: PriorityLevelConfiguration
    metadata:
      name: very-low-priority
    spec:
      limited:
        assuredConcurrencyShares: 5
        limitResponse:
          type: Reject
      type: Limited
    ```

3. Observe the throttled call in the API server metrics.

    ```bash
    kubectl get --raw /metrics | grep "restrict-bad-client"
    ```

[!INCLUDE [Azure Help Support](../../includes/azure-help-support.md)]

<!-- LINKS - external -->
[kube-audit-overview]: https://kubernetes.io/docs/tasks/debug/debug-cluster/audit/
[kube-apiserver-overview]: https://kubernetes.io/docs/reference/command-line-tools-reference/kube-apiserver/
[too-many-requests-error-tsg]: https://learn.microsoft.com/troubleshoot/azure/azure-kubernetes/429-too-many-requests-errors
[monitorr-apiserver]: https://github.com/MicrosoftDocs/azure-docs-pr/blob/main/articles/aks/monitor-apiserver.md
[priority-and-fairness]: https://kubernetes.io/docs/concepts/cluster-administration/flow-control/
[K8s SLOs]: https://github.com/kubernetes/community/blob/master/sig-scalability/slos/slos.md

<!-- LINKS - internal -->
[azure-diagnostics-overview]: ../azure-monitor/essentials/diagnostic-settings.md
[log-analytics-workspace-overview]: /azure/aks/monitor-aks
[design-log-analytics-deployment]: ../azure-monitor/logs/design-logs-deployment.md
[create-diagnostic settings]: ../azure-monitor/essentials/diagnostic-settings.md#create-diagnostic-settings
[cost-optimization-azure-monitor]: ../azure-monitor/best-practices-cost.md
[azure-diagnostics-table]: /azure/azure-monitor/reference/tables/azurediagnostics
[container-insights-overview]: ..//azure-monitor/containers/container-insights-overview.md
[apiserversla]: /azure/aks/free-standard-pricing-tiers#uptime-sla-terms-and-conditions